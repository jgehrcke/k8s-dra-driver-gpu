/*
 * Copyright (c) 2021-2025, NVIDIA CORPORATION.  All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package main

import (
	"fmt"
	"maps"
	"os"
	"os/exec"
	"path/filepath"
	"slices"
	"strings"
	"time"

	"github.com/google/uuid"
	resourceapi "k8s.io/api/resource/v1"
	"k8s.io/klog/v2"

	nvdev "github.com/NVIDIA/go-nvlib/pkg/nvlib/device"
	"github.com/NVIDIA/go-nvlib/pkg/nvpci"
	"github.com/NVIDIA/go-nvml/pkg/nvml"
	"k8s.io/dynamic-resource-allocation/deviceattribute"

	"github.com/NVIDIA/k8s-dra-driver-gpu/pkg/featuregates"
)

type deviceLib struct {
	nvdev.Interface
	nvmllib           nvml.Interface
	nvpci             nvpci.Interface
	driverLibraryPath string
	devRoot           string
	nvidiaSMIPath     string
	gpuInfosByUUID    map[string]*GpuInfo
	gpuUUIDbyMinor    map[GPUMinor]string
	devhandleByUUID   map[string]nvml.Device
}

type GPUMinor = int
type PerGPUMinorAllocatableDevices map[GPUMinor]AllocatableDevices

func newDeviceLib(driverRoot root) (*deviceLib, error) {
	driverLibraryPath, err := driverRoot.getDriverLibraryPath()
	if err != nil {
		return nil, fmt.Errorf("failed to locate driver libraries: %w", err)
	}

	nvidiaSMIPath, err := driverRoot.getNvidiaSMIPath()
	if err != nil {
		return nil, fmt.Errorf("failed to locate nvidia-smi: %w", err)
	}

	// We construct an NVML library specifying the path to libnvidia-ml.so.1
	// explicitly so that we don't have to rely on the library path.
	nvmllib := nvml.New(
		nvml.WithLibraryPath(driverLibraryPath),
	)
	nvpci := nvpci.New()

	d := deviceLib{
		Interface:         nvdev.New(nvmllib),
		nvmllib:           nvmllib,
		driverLibraryPath: driverLibraryPath,
		devRoot:           driverRoot.getDevRoot(),
		nvidiaSMIPath:     nvidiaSMIPath,
		nvpci:             nvpci,
		gpuInfosByUUID:    make(map[string]*GpuInfo),
		gpuUUIDbyMinor:    make(map[GPUMinor]string),
		devhandleByUUID:   make(map[string]nvml.Device),
	}

	if err := d.Init(); err != nil {
		return nil, fmt.Errorf("failed to initialize NVML: %w", err)
	}

	return &d, nil
}

// prependPathListEnvvar prepends a specified list of strings to a specified envvar and returns its value.
func prependPathListEnvvar(envvar string, prepend ...string) string {
	if len(prepend) == 0 {
		return os.Getenv(envvar)
	}
	current := filepath.SplitList(os.Getenv(envvar))
	return strings.Join(append(prepend, current...), string(filepath.ListSeparator))
}

// setOrOverrideEnvvar adds or updates an envar to the list of specified envvars and returns it.
func setOrOverrideEnvvar(envvars []string, key, value string) []string {
	var updated []string
	for _, envvar := range envvars {
		pair := strings.SplitN(envvar, "=", 2)
		if pair[0] == key {
			continue
		}
		updated = append(updated, envvar)
	}
	return append(updated, fmt.Sprintf("%s=%s", key, value))
}

// Getting a device handle by UUID can take O(10) when done concurrently. For
// faster device management, maintain long-term state: initialize once at
// startup, cache handles, serialize NVML calls. TODO: implement a re-init path
// on NVML errors; this hopefully balances performance and robustness for this
// long-running process. Downside: out-of-band NVML clients (e.g., mig-parted)
// may see "in use by another client" errors (they are forbidden by design when
// DynamicMIG is enabled).
func (l deviceLib) Init() error {
	klog.Infof("Initializing NVML")
	ret := l.nvmllib.Init()
	if ret != nvml.SUCCESS {
		return fmt.Errorf("error initializing NVML: %v", ret)
	}
	return nil
}

func (l deviceLib) alwaysShutdown() {
	klog.Infof("Shutting down NVML")
	ret := l.nvmllib.Shutdown()
	if ret != nvml.SUCCESS {
		klog.Warningf("error shutting down NVML: %v", ret)
	}
}

// Discover devices that are allocatable, on this node.
func (l deviceLib) enumerateAllPossibleDevices() (AllocatableDevices, PerGPUMinorAllocatableDevices, error) {
	perGPUAllocatable, err := l.GetPerGpuAllocatableDevices()
	if err != nil {
		return nil, nil, fmt.Errorf("error enumerating allocatable devices: %w", err)
	}

	if featuregates.Enabled(featuregates.PassthroughSupport) {
		// Discover passthrough devices and insert them into the
		// `perGPUAllocatable` map created earlier
		passthroughDevices, err := l.enumerateGpuPciDevices(perGPUAllocatable)
		if err != nil {
			return nil, nil, fmt.Errorf("error enumerating GPU PCI devices: %w", err)
		}
		for minor, ptdevs := range passthroughDevices {
			for name, adev := range ptdevs {
				perGPUAllocatable[minor][name] = adev
			}
		}
	}

	// Flatten `perGPUAllocatable`.
	all := make(AllocatableDevices)
	for _, devices := range perGPUAllocatable {
		maps.Copy(all, devices)
	}

	return all, perGPUAllocatable, nil
}

// GetPerGpuAllocatableDevices() is called once upon startup. It performs device
// discovery, and assembles the set of allocatable devices that will be
// announced by this DRA driver. A list of GPU indices can optionally be
// provided to limit the discovery to a set of physical GPUs.
func (l deviceLib) GetPerGpuAllocatableDevices(indices ...int) (PerGPUMinorAllocatableDevices, error) {
	klog.Infof("Traverse GPU devices")
	perGPUAllocatable := make(PerGPUMinorAllocatableDevices)

	err := l.VisitDevices(func(i int, d nvdev.Device) error {
		if indices != nil && !slices.Contains(indices, i) {
			return nil
		}

		// Prepare data structure for conceptually allocatable devices on this
		// one physical GPU.
		thisGPUAllocatable := make(AllocatableDevices)

		gpuInfo, err := l.getGpuInfo(i, d)
		if err != nil {
			return fmt.Errorf("error getting info for GPU %v: %w", i, err)
		}

		parentdev := &AllocatableDevice{
			Gpu: gpuInfo,
		}

		// Store gpuInfo object for later re-use (lookup by UUID).
		l.gpuInfosByUUID[gpuInfo.UUID] = gpuInfo
		l.gpuUUIDbyMinor[gpuInfo.minor] = gpuInfo.UUID

		// Cache warmup: store mapping between full-GPU UUID and NVML device
		// handle in a map. Ignore failures.
		if _, ret := l.DeviceGetHandleByUUIDCached(gpuInfo.UUID); ret != nvml.SUCCESS {
			klog.Warningf("DeviceGetHandleByUUIDCached failed: %s", ret)
		}

		if featuregates.Enabled(featuregates.DynamicMIG) {
			// For this full device, inspect all MIG profiles and their possible
			// placements. Side effect: this enriches `gpuInfo` with additional
			// properties (such as the memory slice count, and the maximum
			// capacities as reported by individual MIG profiles).
			migspecs, err := l.inspectMigProfilesAndPlacements(gpuInfo, d)
			if err != nil {
				return fmt.Errorf("error getting MIG info for GPU %v: %w", i, err)
			}

			// Announce the full physical GPU.
			thisGPUAllocatable[gpuInfo.CanonicalName()] = parentdev

			for _, migspec := range migspecs {
				dev := &AllocatableDevice{
					MigDynamic: migspec,
				}
				thisGPUAllocatable[migspec.CanonicalName()] = dev
			}

			perGPUAllocatable[gpuInfo.minor] = thisGPUAllocatable

			// Terminate this function -- this is mutually exclusive with static MIG and vfio/passthrough.
			return nil
		}

		migdevs, err := l.discoverMigDevicesByGPU(gpuInfo)
		if err != nil {
			return fmt.Errorf("error discovering MIG devices for GPU %q: %w", gpuInfo.CanonicalName(), err)
		}

		if featuregates.Enabled(featuregates.PassthroughSupport) {
			// Only if no MIG devices are found, allow VFIO devices.
			klog.Infof("PassthroughSupport enabled, and %d MIG devices found", len(migdevs))
			gpuInfo.vfioEnabled = len(migdevs) == 0
		}

		if !gpuInfo.migEnabled {
			klog.Infof("Adding device %s to allocatable devices", gpuInfo.CanonicalName())
			// No static MIG devices prepared for this physical GPU. Announce
			// physical GPU to be allocatable, and terminate discovery for this
			// phyical GPU.
			thisGPUAllocatable[gpuInfo.CanonicalName()] = parentdev
			perGPUAllocatable[gpuInfo.minor] = thisGPUAllocatable
			return nil
		}

		// Process statically pre-configured MIG devices.
		for _, mdev := range migdevs {
			klog.Infof("Adding MIG device %s to allocatable devices (parent: %s)", mdev.CanonicalName(), gpuInfo.CanonicalName())
			thisGPUAllocatable[mdev.CanonicalName()] = mdev
		}

		// Likely unintentionally stranded capacity (misconfiguration).
		if len(migdevs) == 0 {
			klog.Warningf("Physical GPU %s has MIG mode enabled but no configured MIG devices", gpuInfo.CanonicalName())
		}

		perGPUAllocatable[gpuInfo.minor] = thisGPUAllocatable
		return nil
	})

	if err != nil {
		return nil, fmt.Errorf("error visiting devices: %w", err)
	}

	// Should we try to return an object here that retains a stable sort oder
	// over devices, by name? (a normal map does not cut it).
	return perGPUAllocatable, nil
}

func (l deviceLib) discoverMigDevicesByGPU(gpuInfo *GpuInfo) (AllocatableDeviceList, error) {
	var devices AllocatableDeviceList
	migs, err := l.getMigDevices(gpuInfo)
	if err != nil {
		return nil, fmt.Errorf("error getting MIG devices for GPU %q: %w", gpuInfo.CanonicalName(), err)
	}

	for _, migDeviceInfo := range migs {
		mig := &AllocatableDevice{
			MigStatic: migDeviceInfo,
		}
		devices = append(devices, mig)
	}
	return devices, nil
}

// TODO: Need go-nvlib util for this.
func (l deviceLib) discoverGPUByPCIBusID(pcieBusID string) (*AllocatableDevice, AllocatableDeviceList, error) {
	if err := l.Init(); err != nil {
		return nil, nil, err
	}
	defer l.alwaysShutdown()

	var gpu *AllocatableDevice
	var migs AllocatableDeviceList
	err := l.VisitDevices(func(i int, d nvdev.Device) error {
		gpuPCIBusID, err := d.GetPCIBusID()
		if err != nil {
			return fmt.Errorf("error getting PCIe bus ID for device %d: %w", i, err)
		}
		if gpuPCIBusID != pcieBusID {
			return nil
		}
		gpuInfo, err := l.getGpuInfo(i, d)
		if err != nil {
			return fmt.Errorf("error getting info for GPU %d: %w", i, err)
		}
		migs, err = l.discoverMigDevicesByGPU(gpuInfo)
		if err != nil {
			return fmt.Errorf("error discovering MIG devices for GPU %q: %w", gpuInfo.CanonicalName(), err)
		}
		// If no MIG devices are found, allow VFIO devices.
		gpuInfo.vfioEnabled = len(migs) == 0
		gpu = &AllocatableDevice{
			Gpu: gpuInfo,
		}
		return nil
	})
	if err != nil {
		return nil, nil, fmt.Errorf("error visiting devices: %w", err)
	}
	return gpu, migs, nil
}

// TODO: Need go-nvlib util for this.
func (l deviceLib) discoverVfioDevice(gpuInfo *GpuInfo) (*AllocatableDevice, error) {
	gpus, err := l.nvpci.GetGPUs()
	if err != nil {
		return nil, fmt.Errorf("error getting GPU PCI devices: %w", err)
	}
	for idx, gpu := range gpus {
		if gpu.Address != gpuInfo.pcieBusID {
			continue
		}
		vfioDeviceInfo, err := l.getVfioDeviceInfo(idx, gpu)
		if err != nil {
			return nil, fmt.Errorf("error getting VFIO device info: %w", err)
		}
		vfioDeviceInfo.parent = gpuInfo
		return &AllocatableDevice{
			Vfio: vfioDeviceInfo,
		}, nil
	}
	return nil, fmt.Errorf("error discovering VFIO device by PCIe bus ID: %s", gpuInfo.pcieBusID)
}

// Tear down any MIG devices that are present and don't belong to completed claims.
func (l deviceLib) obliterateStaleMIGDevices(expectedDeviceNames []DeviceName) error {
	// if err := l.Init(); err != nil {
	// 	return err
	// }
	// defer l.alwaysShutdown()

	err := l.VisitDevices(func(i int, d nvdev.Device) error {
		ginfo, err := l.getGpuInfo(i, d)
		if err != nil {
			return fmt.Errorf("error getting info for GPU %d: %w", i, err)
		}

		migs, err := l.getMigDevices(ginfo)
		if err != nil {
			return fmt.Errorf("error getting MIG devices for GPU %d: %w", i, err)
		}

		for _, mdi := range migs {
			name := mdi.CanonicalName()
			expected := slices.Contains(expectedDeviceNames, name)
			if !expected {
				klog.Warningf("Found unexpected MIG device (%s), attempt to tear down", name)
				if err := l.deleteMigDevice(mdi.LiveTuple()); err != nil {
					return fmt.Errorf("could not delete unexpected MIG device (%s): %w", name, err)
				}
			}
		}

		return nil
	})

	if err != nil {
		return fmt.Errorf("error visiting devices: %w", err)
	}
	return nil
}

func (l deviceLib) getGpuInfo(index int, device nvdev.Device) (*GpuInfo, error) {
	minor, ret := device.GetMinorNumber()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting minor number for device %d: %v", index, ret)
	}
	uuid, ret := device.GetUUID()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting UUID for device %d: %v", index, ret)
	}
	migEnabled, err := device.IsMigEnabled()
	if err != nil {
		return nil, fmt.Errorf("error checking if MIG mode enabled for device %d: %w", index, err)
	}
	memory, ret := device.GetMemoryInfo()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting memory info for device %d: %v", index, ret)
	}
	productName, ret := device.GetName()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting product name for device %d: %v", index, ret)
	}
	architecture, err := device.GetArchitectureAsString()
	if err != nil {
		return nil, fmt.Errorf("error getting architecture for device %d: %w", index, err)
	}
	brand, err := device.GetBrandAsString()
	if err != nil {
		return nil, fmt.Errorf("error getting brand for device %d: %w", index, err)
	}
	cudaComputeCapability, err := device.GetCudaComputeCapabilityAsString()
	if err != nil {
		return nil, fmt.Errorf("error getting CUDA compute capability for device %d: %w", index, err)
	}
	driverVersion, ret := l.nvmllib.SystemGetDriverVersion()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting driver version: %w", err)
	}
	cudaDriverVersion, ret := l.nvmllib.SystemGetCudaDriverVersion()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting CUDA driver version: %w", err)
	}
	pcieBusID, err := device.GetPCIBusID()
	if err != nil {
		return nil, fmt.Errorf("error getting PCIe bus ID for device %d: %w", index, err)
	}

	// Get the memory-addressing mode supported by the device.
	// On coherent-memory systems, the possible modes are:
	//   - HMM  (Hardware Memory Management)
	//   - ATS  (Address Translation Service)
	//   - None (Supported by the platform but currently inactive)
	//   - ""   (Not supported by the platform)
	var addressingMode *string
	if mode, err := device.GetAddressingModeAsString(); err != nil {
		return nil, fmt.Errorf("error getting addressing mode for device %d: %w", index, err)
	} else if mode != "" {
		addressingMode = &mode
	}

	var pcieRootAttr *deviceattribute.DeviceAttribute
	if attr, err := deviceattribute.GetPCIeRootAttributeByPCIBusID(pcieBusID); err == nil {
		pcieRootAttr = &attr
	} else {
		klog.Warningf("error getting PCIe root for device %d, continuing without attribute: %v", index, err)
	}

	var migProfiles []*MigProfileInfo
	for i := 0; i < nvml.GPU_INSTANCE_PROFILE_COUNT; i++ {
		giProfileInfo, ret := device.GetGpuInstanceProfileInfo(i)
		if ret == nvml.ERROR_NOT_SUPPORTED {
			continue
		}
		if ret == nvml.ERROR_INVALID_ARGUMENT {
			continue
		}
		if ret != nvml.SUCCESS {
			return nil, fmt.Errorf("error retrieving GpuInstanceProfileInfo for profile %d on GPU %v", i, uuid)
		}

		giPossiblePlacements, ret := device.GetGpuInstancePossiblePlacements(&giProfileInfo)
		if ret == nvml.ERROR_NOT_SUPPORTED {
			continue
		}
		if ret == nvml.ERROR_INVALID_ARGUMENT {
			continue
		}
		if ret != nvml.SUCCESS {
			return nil, fmt.Errorf("error retrieving GpuInstancePossiblePlacements for profile %d on GPU %v", i, uuid)
		}

		var migDevicePlacements []*MigDevicePlacement
		for _, p := range giPossiblePlacements {
			mdp := &MigDevicePlacement{
				GpuInstancePlacement: p,
			}
			migDevicePlacements = append(migDevicePlacements, mdp)
		}

		for j := 0; j < nvml.COMPUTE_INSTANCE_PROFILE_COUNT; j++ {
			for k := 0; k < nvml.COMPUTE_INSTANCE_ENGINE_PROFILE_COUNT; k++ {
				migProfile, err := l.NewMigProfile(i, j, k, giProfileInfo.MemorySizeMB, memory.Total)
				if err != nil {
					return nil, fmt.Errorf("error building MIG profile from GpuInstanceProfileInfo for profile %d on GPU %v", i, uuid)
				}

				if migProfile.GetInfo().G != migProfile.GetInfo().C {
					continue
				}

				profileInfo := &MigProfileInfo{
					profile:    migProfile,
					placements: migDevicePlacements,
				}

				migProfiles = append(migProfiles, profileInfo)
			}
		}
	}

	gpuInfo := &GpuInfo{
		UUID:                  uuid,
		minor:                 minor,
		migEnabled:            migEnabled,
		memoryBytes:           memory.Total,
		productName:           productName,
		brand:                 brand,
		architecture:          architecture,
		cudaComputeCapability: cudaComputeCapability,
		driverVersion:         driverVersion,
		cudaDriverVersion:     fmt.Sprintf("%v.%v", cudaDriverVersion/1000, (cudaDriverVersion%1000)/10),
		pcieBusID:             pcieBusID,
		pcieRootAttr:          pcieRootAttr,
		migProfiles:           migProfiles,
		Health:                Healthy,
		addressingMode:        addressingMode,
	}

	return gpuInfo, nil
}

func (l deviceLib) enumerateGpuPciDevices(devs PerGPUMinorAllocatableDevices) (PerGPUMinorAllocatableDevices, error) {
	perGPUAllocatable := make(PerGPUMinorAllocatableDevices)

	// Input `devs` contains all devices discovered for announcement so far,
	// grouped by GPU minor. Flatten input (`devs`) into a list.
	all := make(AllocatableDevices)
	for _, devices := range perGPUAllocatable {
		maps.Copy(all, devices)
	}

	// Discover PCI devices.
	gpuPciDevices, err := l.nvpci.GetGPUs()
	if err != nil {
		return nil, fmt.Errorf("error getting GPU PCI devices: %w", err)
	}

	// For each discovered PCI device, look up the corresponding full-GPU-device
	// from `adevs`. Construct a corresponding new `AllocatableDevice` object.
	for idx, pci := range gpuPciDevices {
		thisGPUAllocatable := make(AllocatableDevices)
		parent := all.GetGPUByPCIeBusID(pci.Address)

		if parent == nil || !parent.Gpu.vfioEnabled {
			continue
		}

		vfioDeviceInfo, err := l.getVfioDeviceInfo(idx, pci)
		if err != nil {
			return nil, fmt.Errorf("error getting GPU info from PCI device: %w", err)
		}
		vfioDeviceInfo.parent = parent.Gpu

		thisGPUAllocatable[vfioDeviceInfo.CanonicalName()] = &AllocatableDevice{
			Vfio: vfioDeviceInfo,
		}

		perGPUAllocatable[parent.Gpu.minor] = thisGPUAllocatable
	}

	return perGPUAllocatable, nil
}

func (l deviceLib) getVfioDeviceInfo(idx int, device *nvpci.NvidiaPCIDevice) (*VfioDeviceInfo, error) {
	var pcieRootAttr *deviceattribute.DeviceAttribute
	attr, err := deviceattribute.GetPCIeRootAttributeByPCIBusID(device.Address)
	if err == nil {
		pcieRootAttr = &attr
	} else {
		klog.Warningf("error getting PCIe root for device %s, continuing without attribute: %v", device.Address, err)
	}

	_, memoryBytes := device.Resources.GetTotalAddressableMemory(true)

	vfioDeviceInfo := &VfioDeviceInfo{
		UUID:                   uuid.NewSHA1(uuid.NameSpaceDNS, []byte(device.Address)).String(),
		index:                  idx,
		productName:            device.DeviceName,
		pcieBusID:              device.Address,
		pcieRootAttr:           pcieRootAttr,
		deviceID:               fmt.Sprintf("0x%04x", device.Device),
		vendorID:               fmt.Sprintf("0x%04x", device.Vendor),
		numaNode:               device.NumaNode,
		iommuGroup:             device.IommuGroup,
		addressableMemoryBytes: memoryBytes,
	}

	return vfioDeviceInfo, nil
}

func (l deviceLib) getMigDevices(gpuInfo *GpuInfo) (map[string]*MigDeviceInfo, error) {
	if !gpuInfo.migEnabled {
		return nil, nil
	}

	device, ret := l.DeviceGetHandleByUUIDCached(gpuInfo.UUID)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting GPU device handle: %v", ret)
	}

	infos := make(map[string]*MigDeviceInfo)
	err := walkMigDevices(device, func(i int, migDevice nvml.Device) error {
		giID, ret := migDevice.GetGpuInstanceId()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting GPU instance ID for MIG device: %v", ret)
		}
		gi, ret := device.GetGpuInstanceById(giID)
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting GPU instance for '%v': %v", giID, ret)
		}
		giInfo, ret := gi.GetInfo()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting GPU instance info for '%v': %v", giID, ret)
		}
		ciID, ret := migDevice.GetComputeInstanceId()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting Compute instance ID for MIG device: %v", ret)
		}
		ci, ret := gi.GetComputeInstanceById(ciID)
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting Compute instance for '%v': %v", ciID, ret)
		}
		ciInfo, ret := ci.GetInfo()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting Compute instance info for '%v': %v", ciID, ret)
		}
		uuid, ret := migDevice.GetUUID()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting UUID for MIG device: %v", ret)
		}

		var migProfile *MigProfileInfo
		var giProfileInfo *nvml.GpuInstanceProfileInfo
		var ciProfileInfo *nvml.ComputeInstanceProfileInfo
		for _, profile := range gpuInfo.migProfiles {
			profileInfo := profile.profile.GetInfo()
			gipInfo, ret := device.GetGpuInstanceProfileInfo(profileInfo.GIProfileID)
			if ret != nvml.SUCCESS {
				continue
			}
			if giInfo.ProfileId != gipInfo.Id {
				continue
			}
			cipInfo, ret := gi.GetComputeInstanceProfileInfo(profileInfo.CIProfileID, profileInfo.CIEngProfileID)
			if ret != nvml.SUCCESS {
				continue
			}
			if ciInfo.ProfileId != cipInfo.Id {
				continue
			}
			migProfile = profile
			giProfileInfo = &gipInfo
			ciProfileInfo = &cipInfo
		}
		if migProfile == nil {
			return fmt.Errorf("error getting profile info for MIG device: %v", uuid)
		}

		placement := MigDevicePlacement{
			GpuInstancePlacement: giInfo.Placement,
		}

		infos[uuid] = &MigDeviceInfo{
			UUID:           uuid,
			Profile:        migProfile.String(),
			ParentMinor:    gpuInfo.minor,
			ParentUUID:     gpuInfo.UUID,
			CIID:           int(ciInfo.Id),
			GIID:           int(giInfo.Id),
			PlacementStart: int(placement.Start),
			PlacementSize:  int(placement.Size),
			GiProfileID:    int(giProfileInfo.Id),
			parent:         gpuInfo,
			giProfileInfo:  giProfileInfo,
			gIInfo:         &giInfo,
			ciProfileInfo:  ciProfileInfo,
			cIInfo:         &ciInfo,
			pcieBusID:      gpuInfo.pcieBusID,
			pcieRootAttr:   gpuInfo.pcieRootAttr,
			Health:         Healthy,
		}
		return nil
	})
	if err != nil {
		return nil, fmt.Errorf("error enumerating MIG devices: %w", err)
	}

	if len(infos) == 0 {
		return nil, nil
	}

	return infos, nil
}

func walkMigDevices(d nvml.Device, f func(i int, d nvml.Device) error) error {
	count, ret := nvml.Device(d).GetMaxMigDeviceCount()
	if ret != nvml.SUCCESS {
		return fmt.Errorf("error getting max MIG device count: %v", ret)
	}

	for i := 0; i < count; i++ {
		device, ret := d.GetMigDeviceHandleByIndex(i)
		if ret == nvml.ERROR_NOT_FOUND {
			continue
		}
		if ret == nvml.ERROR_INVALID_ARGUMENT {
			continue
		}
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting MIG device handle at index '%v': %v", i, ret)
		}
		err := f(i, device)
		if err != nil {
			return err
		}
	}
	return nil
}

func (l deviceLib) setTimeSlice(uuids []string, timeSlice int) error {
	for _, uuid := range uuids {
		cmd := exec.Command(
			l.nvidiaSMIPath,
			"compute-policy",
			"-i", uuid,
			"--set-timeslice", fmt.Sprintf("%d", timeSlice))

		// In order for nvidia-smi to run, we need update LD_PRELOAD to include the path to libnvidia-ml.so.1.
		cmd.Env = setOrOverrideEnvvar(os.Environ(), "LD_PRELOAD", prependPathListEnvvar("LD_PRELOAD", l.driverLibraryPath))

		output, err := cmd.CombinedOutput()
		if err != nil {
			klog.Errorf("\n%v", string(output))
			return fmt.Errorf("error running nvidia-smi: %w", err)
		}
	}
	return nil
}

func (l deviceLib) setComputeMode(uuids []string, mode string) error {
	for _, uuid := range uuids {
		cmd := exec.Command(
			l.nvidiaSMIPath,
			"-i", uuid,
			"-c", mode)

		// In order for nvidia-smi to run, we need update LD_PRELOAD to include the path to libnvidia-ml.so.1.
		cmd.Env = setOrOverrideEnvvar(os.Environ(), "LD_PRELOAD", prependPathListEnvvar("LD_PRELOAD", l.driverLibraryPath))

		output, err := cmd.CombinedOutput()
		if err != nil {
			klog.Errorf("\n%v", string(output))
			return fmt.Errorf("error running nvidia-smi: %w", err)
		}
	}
	return nil
}

// This is meant to only be used for physical, full GPUs (not MIG devices).
func (l deviceLib) DeviceGetHandleByUUIDCached(uuid string) (nvml.Device, nvml.Return) {
	dev, exists := l.devhandleByUUID[uuid]
	if exists {
		return dev, nvml.SUCCESS
	}

	klog.V(6).Infof("DeviceGetHandleByUUIDCached called for %s, cache miss", uuid)
	// Note(JP): under pressure, this call can be slow. Hence, the decision to
	// use long-lived handles (at least for DynamicMIG). In theory here we need
	// a request coalescing strategy (otherwise, cache stampede is a thing in
	// practice: a burst of requests with the same UUID might be incoming in a
	// timeframe much shorter than it takes for the call below to succeed. All
	// cache misses then end up doing this expensive lookup, although it only
	// needs to be performed once). For now, I opt for addressing this by
	// warming up the cache during program startup. Given that the set of full
	// GPUs is static and that we currently have no expiry (but a long-lived
	// map), that will work.
	t0 := time.Now()
	dev, ret := l.nvmllib.DeviceGetHandleByUUID(uuid)
	klog.V(6).Infof("t_device_get_handle_by_uuid %.3f s", time.Since(t0).Seconds())

	if ret != nvml.SUCCESS {
		return nil, ret
	}

	// Pulate `devhandleByUUID` and `devhandleByMinor` maps for fast handle
	// lookup.
	l.devhandleByUUID[uuid] = dev
	return dev, ret
}

func (l deviceLib) createMigDevice(migspec *MigSpec) (*MigDeviceInfo, error) {
	gpu := migspec.Parent
	profile := migspec.Profile
	placement := &migspec.Placement

	// tcmigdev0 := time.Now()
	// if err := l.Init(); err != nil {
	// 	return nil, err
	// }
	// defer l.alwaysShutdown()
	// klog.V(6).Infof("t_prep_create_mig_dev_init %.3f s", time.Since(tcmigdev0).Seconds())

	tdhbu0 := time.Now()
	// I've seen this to take up to 10 seconds, also four six 8 seconds.
	// Can this be cached?
	//device, ret := l.nvmllib.DeviceGetHandleByUUID(gpu.UUID)
	device, ret := l.DeviceGetHandleByUUIDCached(gpu.UUID)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting GPU device handle: %v", ret)
	}
	klog.V(7).Infof("t_prep_create_mig_dev_get_dev_handle %.3f s", time.Since(tdhbu0).Seconds())

	tnd0 := time.Now()
	ndev, err := l.NewDevice(device)
	if err != nil {
		return nil, fmt.Errorf("error instantiating nvml dev: %w", err)
	}
	klog.V(7).Infof("t_prep_create_mig_dev_new_dev %.3f s", time.Since(tnd0).Seconds())

	// nvml GetMigMode distinguishes between current and pending -- not exposed
	// in go-nvlib yet. Maybe that distinction is important here.
	// migModeCurrent, migModePending, err := device.GetMigMode()
	// https://github.com/NVIDIA/go-nvlib/blame/7d260da4747c220a6972ebc83e4eb7116fc9b89a/pkg/nvlib/device/device.go#L225
	tcme0 := time.Now()
	migEnabled, err := ndev.IsMigEnabled()
	if err != nil {
		return nil, fmt.Errorf("error checking if MIG mode enabled for device %s: %w", ndev, err)
	}
	klog.V(7).Infof("t_prep_create_mig_dev_check_mig_enabled %.3f s", time.Since(tcme0).Seconds())

	logpfx := fmt.Sprintf("Create %s", migspec.CanonicalName())

	if !migEnabled {
		klog.V(6).Infof("%s: Attempting to enable MIG mode for to-be parent %s", logpfx, gpu.String())
		// If this is newer than A100 and if device unused: enable MIG.
		tem0 := time.Now()
		ret, activationStatus := device.SetMigMode(nvml.DEVICE_MIG_ENABLE)
		if ret != nvml.SUCCESS {
			// activationStatus would return the appropriate error code upon unsuccessful activation
			klog.Warningf("%s: SetMigMode activationStatus (device %s): %s", logpfx, gpu.String(), activationStatus)
			return nil, fmt.Errorf("error enabling MIG mode for device %s: %v", gpu.String(), ret)
		}
		klog.V(1).Infof("%s: MIG mode now enabled for device %s, t_enable_mig %.3f s", logpfx, gpu.String(), time.Since(tem0).Seconds())
	} else {
		klog.V(6).Infof("%s: MIG mode already enabled for device %s", logpfx, gpu.String())
	}

	profileInfo := profile.GetInfo()

	tcgigi0 := time.Now()
	giProfileInfo, ret := device.GetGpuInstanceProfileInfo(profileInfo.GIProfileID)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting GPU instance profile info for '%v': %v", profile, ret)
	}

	gi, ret := device.CreateGpuInstanceWithPlacement(&giProfileInfo, placement)

	// Ambiguity in the NVML API: when requesting a specific placement that is
	// already occupied, NVML returns NVML_ERROR_INSUFFICIENT_RESOURCES rather
	// than NVML_ERROR_ALREADY_EXISTS. Seemingly, to robustly distinguish
	// "already exists" from "blocked by something else", one cannot rely on the
	// error code alone. One must check the device state. Unrelatedly, if this
	// GPU instance already exists, it is unclear if we can and should safely
	// proceed using it. When we're here, we're not expecting it to exist -- an
	// active destruction should be performed before retrying creation. Hence,
	// for now, just return an error without distinguishing "already exists"
	// from any other type of fault.
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error creating GPU instance for '%s': %v", migspec.CanonicalName(), ret)
	}

	giInfo, ret := gi.GetInfo()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting GPU instance info for '%s': %v", migspec.CanonicalName(), ret)
	}

	ciProfileInfo, ret := gi.GetComputeInstanceProfileInfo(profileInfo.CIProfileID, profileInfo.CIEngProfileID)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting Compute instance profile info for '%v': %v", profile, ret)
	}

	ci, ret := gi.CreateComputeInstance(&ciProfileInfo)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error creating Compute instance for '%v': %v", profile, ret)
	}

	ciInfo, ret := ci.GetInfo()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting GPU instance info for '%v': %v", profile, ret)
	}
	klog.V(6).Infof("t_prep_create_mig_dev_cigi %.3f s", time.Since(tcgigi0).Seconds())

	// Note(JP): for obtaining the UUID of the just-created MIG device, some
	// algorithms walk through all MIG devices on the parent GPU to identify the
	// one that matches the CIID and GIID of the MIG device that was just
	// created. While that is correct, I measured that the time spent in NVML
	// API calls for 'walking all MIG devices' under load under can easily be
	// O(10 s). The UUID can also be obtained by first getting the MIG device
	// handle from the CI and then calling GetUUID() on that handle. A MIG
	// device handle maps 1:1 to a CI in NVML, so once the CI is known, the MIG
	// device handle and its UUID can be retrieved directly without scanning
	// through indices.
	uuid, ret := ciInfo.Device.GetUUID()
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("error getting UUID from CI info/device for CI %d: %v", ciInfo.Id, ret)
	}

	// This now probably needs consolidation with the new types MigLiveTuple and
	// MigSpecTuple. Things get confusing.
	migDevInfo := &MigDeviceInfo{
		UUID:           uuid,
		CIID:           int(ciInfo.Id),
		GIID:           int(giInfo.Id),
		ParentMinor:    gpu.minor,
		ParentUUID:     gpu.UUID,
		Profile:        profile.String(),
		PlacementStart: int(placement.Start),
		PlacementSize:  int(placement.Size),
		GiProfileID:    int(giProfileInfo.Id),
		gIInfo:         &giInfo,
		cIInfo:         &ciInfo,
		parent:         gpu,
	}

	klog.V(6).Infof("%s: MIG device created on %s: %s (%+v)", logpfx, gpu.String(), migDevInfo.CanonicalName(), migDevInfo.LiveTuple())
	return migDevInfo, nil
}

func (l deviceLib) deleteMigDevice(miglt *MigLiveTuple) error {
	parentUUID := l.gpuUUIDbyMinor[miglt.ParentMinor]
	giId := miglt.GIID
	ciId := miglt.CIID

	t0 := time.Now()
	migStr := fmt.Sprintf("MIG(parent: %s, %+v)", parentUUID, miglt)
	klog.V(6).Infof("Delete %s", migStr)

	// if err := l.Init(); err != nil {
	// 	return err
	// }
	// defer l.alwaysShutdown()

	//parentNvmlDev, ret := l.nvmllib.DeviceGetHandleByUUID(parentUUID)
	parentNvmlDev, ret := l.DeviceGetHandleByUUIDCached(parentUUID)
	if ret != nvml.SUCCESS {
		return fmt.Errorf("error getting device from UUID '%v': %v", parentUUID, ret)
	}

	// The order of destroying 1) compute instance and 2) GPU instance matters.
	// These resources are hierarchical: compute instances are created inside a
	// GPU instance, so the parent GPU instance cannot be destroyed while
	// children (compute instances) still exist.
	gi, gires := parentNvmlDev.GetGpuInstanceById(giId)

	// Ref docs document this error with "If device doesn't have MIG mode
	// enabled" -- for the unlikely case that we end up in this state (MIG mode
	// was disabled out-of-band?), this should be treated as deletion success.
	if gires == nvml.ERROR_NOT_SUPPORTED {
		klog.Infof("Delete %s: GetGpuInstanceById yielded ERROR_NOT_SUPPORTED: MIG disabled, treat as success", migStr)
		return nil
	}

	// UNINITIALIZED, INVALID_ARGUMENT, NO_PERMISSION
	if gires != nvml.SUCCESS && gires != nvml.ERROR_NOT_FOUND {
		return fmt.Errorf("error getting GPU instance handle for MIG device: %v", ret)
	}

	if gires == nvml.ERROR_NOT_FOUND {
		// In this case assume that no compute instances exist (as of the GI>CI
		// hierarchy) and proceed with attempt-to-disable-MIG-mode
		klog.Infof("Delete %s: GI was not found skip CI cleanup", migStr)
		if err := l.maybeDisableMigMode(parentUUID, parentNvmlDev); err != nil {
			return fmt.Errorf("failed maybeDisableMigMode: %w", err)
		}
		return nil
	}

	// Remainder, with `gi` actually being valid.
	ci, cires := gi.GetComputeInstanceById(ciId)

	// Here we could compare the actual MIG UUID with an expected MIG UUID,
	// to be extra sure that this we want to proceed with deletion.
	// ciInfo, res := ci.GetInfo()
	// if res != nvml.SUCCESS {
	// 	return fmt.Errorf("error calling ci.GetInfo(): %v", ret)
	// }

	// actualMigUUID, res := nvml.DeviceGetUUID(ciInfo.Device)
	// if res != nvml.SUCCESS {
	// 	return fmt.Errorf("nvml.DeviceGetUUID() failed: %v", ret)
	// }
	// if actualMigUUID != expectedMigUUID {
	// 	return fmt.Errorf("UUID mismatch upon deletion: expected: %s actual: %s", expectedMigUUID, actualMigUUID)
	// }

	// Can never be `ERROR_NOT_SUPPORTED` at this point. Can be UNINITIALIZED,
	// INVALID_ARGUMENT, NO_PERMISSION: for those three, it's worth erroring out
	// here (to be retried later).
	if cires != nvml.SUCCESS && cires != nvml.ERROR_NOT_FOUND {
		return fmt.Errorf("error getting Compute instance handle for MIG device %s: %v", migStr, ret)
	}

	// A previous, partial cleanup may actually have already deleted that. Seen
	// in practice. Ignore, and proceed with deleting GPU instance below.
	if cires == nvml.ERROR_NOT_FOUND {
		klog.Infof("Delete %s: CI not found, ignore", migStr)
	} else {
		ret := ci.Destroy()
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error destroying Compute instance: %v", ret)
		}
	}

	// That can for example fail with "In use by another client", in which case
	// we may have performed only a partial cleanup (CI already destroyed; seen
	// in practice).

	// Note that this operation may take O(1 s). In a machine supporting many
	// MIG devices and significant job throughput, this may become noticeable.
	// In a stressing test, I have seen the prep/unprep lock acquisition time
	// out after 10 seconds, when requests pile up.
	ret = gi.Destroy()
	if ret != nvml.SUCCESS {
		return fmt.Errorf("error destroying GPU Instance: %v", ret)
	}
	klog.V(6).Infof("t_delete_mig_device %.3f s", time.Since(t0).Seconds())

	if err := l.maybeDisableMigMode(parentUUID, parentNvmlDev); err != nil {
		return fmt.Errorf("failed maybeDisableMigMode: %w", err)
	}

	return nil
}

func (l deviceLib) maybeDisableMigMode(uuid string, nvmldev nvml.Device) error {
	// Expect the parent GPU to be represented in in `l.gpuInfosByUUID`
	gpu, ok := l.gpuInfosByUUID[uuid]
	if !ok {
		// TODO: this is a programming error -- panic instead
		return fmt.Errorf("uuid not in gpuInfosByUUID: %s", uuid)
	}

	migs, err := l.getMigDevices(gpu)
	if err != nil {
		return fmt.Errorf("error getting MIG devices for %s: %w", gpu.String(), err)
	}

	if len(migs) > 0 {
		klog.V(6).Infof("Leaving MIG mode enabled for device %s (currently present MIG devices: %d)", gpu.String(), len(migs))
		return nil
	}

	klog.V(6).Infof("Attempting to disable MIG mode for device %s", gpu.String())
	t0 := time.Now()
	ret, activationStatus := nvmldev.SetMigMode(nvml.DEVICE_MIG_DISABLE)
	klog.V(6).Infof("t_disable_mig %.3f s", time.Since(t0).Seconds())
	if ret != nvml.SUCCESS {
		// activationStatus would return the appropriate error code upon unsuccessful activation
		klog.Warningf("SetMigMode activationStatus (device %s): %s", gpu.String(), activationStatus)
		// We could also log this as an error and proceed, and hope for the
		// state machine to clean this up in the future. Probably not a good
		// idea.
		return fmt.Errorf("error disabling MIG mode for device %s: %v", gpu.String(), ret)
	}
	klog.V(1).Infof("MIG mode now disabled for device %s", gpu.String())
	return nil
}

// Returns a flat list of all possible physical MIG configurations for a
// specific GPU. Specifically, this discovers all possible profiles, and then
// then determines the possible placements for each profile.
func (l deviceLib) inspectMigProfilesAndPlacements(gpuInfo *GpuInfo, device nvdev.Device) ([]*MigSpec, error) {
	var infos []*MigSpec

	maxCapacities := make(PartCapacityMap)
	maxMemSlicesConsumed := 0

	err := device.VisitMigProfiles(func(migProfile nvdev.MigProfile) error {
		if migProfile.GetInfo().C != migProfile.GetInfo().G {
			return nil
		}

		if migProfile.GetInfo().CIProfileID == nvml.COMPUTE_INSTANCE_PROFILE_1_SLICE_REV1 {
			return nil
		}

		giProfileInfo, ret := device.GetGpuInstanceProfileInfo(migProfile.GetInfo().GIProfileID)
		if ret == nvml.ERROR_NOT_SUPPORTED {
			return nil
		}
		if ret == nvml.ERROR_INVALID_ARGUMENT {
			return nil
		}
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting GI Profile info for MIG profile %v: %w", migProfile, ret)
		}

		giPlacements, ret := device.GetGpuInstancePossiblePlacements(&giProfileInfo)
		if ret == nvml.ERROR_NOT_SUPPORTED {
			return nil
		}
		if ret == nvml.ERROR_INVALID_ARGUMENT {
			return nil
		}
		if ret != nvml.SUCCESS {
			return fmt.Errorf("error getting GI possible placements for MIG profile %v: %w", migProfile, ret)
		}

		for _, giPlacement := range giPlacements {
			mi := &MigSpec{
				Parent:        gpuInfo,
				Profile:       migProfile,
				GIProfileInfo: giProfileInfo,
				Placement:     giPlacement,
			}
			infos = append(infos, mi)

			// Assume that the largest MIG profile consumes all memory slices,
			// and hence we can infer the memory slice count by looking at the
			// Size property of all MigPP objects, and picking the maximum.
			maxMemSlicesConsumed = max(maxMemSlicesConsumed, int(giPlacement.Size))

			// Across all MIG profiles, identify the largest value for each
			// capacity dimension. They probably all corresponding to the same
			// profile.
			caps := mi.PartCapacities()
			for name, cap := range caps {
				setMax(maxCapacities, name, cap)
			}
		}
		return nil
	})

	klog.V(1).Infof("%s: Per-capacity maximum across all MIG profiles+placements: %v", gpuInfo.String(), maxCapacities)
	klog.V(1).Infof("%s: Largest MIG placement size seen (maxMemSlicesConsumed): %d", gpuInfo.String(), maxMemSlicesConsumed)

	if err != nil {
		return nil, fmt.Errorf("error visiting MIG profiles: %w", err)
	}

	// Mutate the full-device information container `gpuInfo`; enrich it with
	// detail obtained from walking MIG devices. Assume that the largest MIG
	// profile seen consumes all memory slices; equate maxMemSlicesConsumed =
	// memSliceCount.
	gpuInfo.AddDetailAfterWalkingMigProfiles(maxCapacities, maxMemSlicesConsumed)
	return infos, nil
}

// FindMigDevBySpec() tests if a MIG device defined by the provided
// `MigSpecTuple` exists. If it exists, a pointer to a corresponding
// `MigLiveTuple` is returned. If it doesn't exist, a nil pointer is returned.
// if an NVML API call fails along the way, a nil pointer and a non-nil error is
// returned.
func (l deviceLib) FindMigDevBySpec(ms *MigSpecTuple) (*MigLiveTuple, error) {
	parentUUID := l.gpuUUIDbyMinor[ms.ParentMinor]
	parent, ret := l.DeviceGetHandleByUUIDCached(parentUUID)
	if ret != nvml.SUCCESS {
		return nil, fmt.Errorf("could not get device handle by UUID for %s", parentUUID)
	}

	count, _ := parent.GetMaxMigDeviceCount()

	for i := range count {
		migHandle, ret := parent.GetMigDeviceHandleByIndex(i)
		if ret != nvml.SUCCESS {
			klog.Infof("GetMigDeviceHandleByIndex ret not success")
			// Slot empty or invalid: treat as device does not currently exist.
			continue
		}

		giId, ret := migHandle.GetGpuInstanceId()
		if ret != nvml.SUCCESS {
			return nil, fmt.Errorf("failed to get GI ID: %v", ret)
		}

		giHandle, ret := parent.GetGpuInstanceById(giId)
		if ret != nvml.SUCCESS {
			return nil, fmt.Errorf("failed to get GI handle for ID %d: %v", giId, ret)
		}

		giInfo, ret := giHandle.GetInfo()
		if ret != nvml.SUCCESS {
			return nil, fmt.Errorf("failed to get GI info: %v", ret)
		}

		klog.V(7).Infof("FindMigDevBySpec: saw MIG dev with profile id %d and placement start %d", giInfo.ProfileId, giInfo.Placement.Start)

		if giInfo.ProfileId != uint32(ms.ProfileID) {
			klog.V(7).Infof("profile ID mismatch: looking for %d", uint32(ms.ProfileID))
			continue
		}

		if giInfo.Placement.Start != uint32(ms.PlacementStart) {
			klog.V(7).Infof("placement start mismatch: looking for %d", ms.PlacementStart)
			continue
		}

		klog.V(4).Infof("FindMigDevBySpec: match found for profile ID %d and placement start %d", giInfo.ProfileId, giInfo.Placement.Start)

		// In our way of managing MIG devices, this should always be zero --
		// nevertheless, perform the lookup. If the lookup fails, the MIG device
		// may have been created only partially (only GI, but not CI). In that
		// case, proceed.
		ciId := 0
		ciId, ret = migHandle.GetComputeInstanceId()
		if ret != nvml.SUCCESS {
			klog.V(4).Infof("FindMigDevBySpec(): failed to get CI ID: %v", ret)
			// return nil, fmt.Errorf("failed to get CI ID: %v", ret)
		}

		uuid := ""
		uuid, ret = migHandle.GetUUID()
		if ret != nvml.SUCCESS {
			klog.V(4).Infof("FindMigDevBySpec(): failed to get MIG UUID: %v", ret)
			// return nil, fmt.Errorf("failed to get MIG UUID: %v", ret)
		}

		// Found device matching the spec, return handle. For subsequent
		// deletion of a potentially partially prepared MIG device, it is OK if
		// CIID and uuid are zero values.
		mlt := MigLiveTuple{
			ParentMinor: ms.ParentMinor,
			GIID:        giId,
			CIID:        ciId,
			UUID:        uuid,
		}

		klog.Infof("FindMigDevBySpec result: %+v", mlt)
		return &mlt, nil
	}

	klog.Infof("Iterated through all potential MIG devs -- no candidate found")
	return nil, nil
}

// Mutate map `m` in-place: insert into map if the current QualifiedName does
// not yet exist as a key. Otherwise, update item in map if the incoming value
// `v` is larger than the one currently stored in the map.
func setMax(m map[resourceapi.QualifiedName]resourceapi.DeviceCapacity, k resourceapi.QualifiedName, v resourceapi.DeviceCapacity) {
	if cur, ok := m[k]; !ok || v.Value.Value() > cur.Value.Value() {
		m[k] = v
	}
}
