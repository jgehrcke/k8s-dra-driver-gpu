/*
 * Copyright (c) 2022-2023, NVIDIA CORPORATION.  All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package main

import (
	"fmt"
	"io"
	"time"

	"github.com/sirupsen/logrus"

	nvdevice "github.com/NVIDIA/go-nvlib/pkg/nvlib/device"
	"github.com/NVIDIA/go-nvml/pkg/nvml"
	"github.com/NVIDIA/nvidia-container-toolkit/pkg/nvcdi"
	"github.com/NVIDIA/nvidia-container-toolkit/pkg/nvcdi/spec"
	transformroot "github.com/NVIDIA/nvidia-container-toolkit/pkg/nvcdi/transform/root"
	"k8s.io/klog/v2"

	utilcache "k8s.io/apimachinery/pkg/util/cache"
	cdiapi "tags.cncf.io/container-device-interface/pkg/cdi"
	cdiparser "tags.cncf.io/container-device-interface/pkg/parser"
	cdispec "tags.cncf.io/container-device-interface/specs-go"

	"github.com/NVIDIA/k8s-dra-driver-gpu/internal/common"
)

const (
	cdiVendor      = "k8s." + DriverName
	cdiClaimClass  = "claim"
	defaultCDIRoot = "/var/run/cdi"
	procNvCapsPath = "/proc/driver/nvidia/capabilities"
)

type CDIHandler struct {
	logger            *logrus.Logger
	nvml              nvml.Interface
	nvdevice          nvdevice.Interface
	nvcdiClaim        nvcdi.Interface
	cache             *cdiapi.Cache
	driverRoot        string
	devRoot           string
	targetDriverRoot  string
	nvidiaCDIHookPath string

	specCache *utilcache.Expiring

	cdiRoot    string
	vendor     string
	claimClass string
}

func NewCDIHandler(opts ...cdiOption) (*CDIHandler, error) {
	h := &CDIHandler{}
	for _, opt := range opts {
		opt(h)
	}

	if h.logger == nil {
		h.logger = logrus.New()
		h.logger.SetOutput(io.Discard)
	}
	if h.nvml == nil {
		h.nvml = nvml.New()
	}
	if h.cdiRoot == "" {
		h.cdiRoot = defaultCDIRoot
	}
	if h.nvdevice == nil {
		h.nvdevice = nvdevice.New(h.nvml)
	}
	if h.vendor == "" {
		h.vendor = cdiVendor
	}
	if h.claimClass == "" {
		h.claimClass = cdiClaimClass
	}

	if h.nvcdiClaim == nil {
		nvcdilib, err := nvcdi.New(
			nvcdi.WithDeviceLib(h.nvdevice),
			nvcdi.WithDriverRoot(h.driverRoot),
			nvcdi.WithDevRoot(h.devRoot),
			nvcdi.WithLogger(h.logger),
			nvcdi.WithNvmlLib(h.nvml),
			nvcdi.WithMode("nvml"),
			nvcdi.WithVendor(h.vendor),
			nvcdi.WithClass(h.claimClass),
			nvcdi.WithNVIDIACDIHookPath(h.nvidiaCDIHookPath),
			nvcdi.WithFeatureFlags(nvcdi.FeatureDisableNvsandboxUtils),
		)
		if err != nil {
			return nil, fmt.Errorf("unable to create CDI library for claims: %w", err)
		}
		h.nvcdiClaim = nvcdilib
	}
	if h.cache == nil {
		cache, err := cdiapi.NewCache(
			cdiapi.WithSpecDirs(h.cdiRoot),
		)
		if err != nil {
			return nil, fmt.Errorf("unable to create a new CDI cache: %w", err)
		}
		h.cache = cache
	}

	h.specCache = utilcache.NewExpiring()

	return h, nil
}

func (cdi *CDIHandler) writeSpec(spec spec.Interface, specName string) error {
	// Transform the spec to make it aware that it is running inside a container.
	err := transformroot.New(
		transformroot.WithRoot(cdi.driverRoot),
		transformroot.WithTargetRoot(cdi.targetDriverRoot),
		transformroot.WithRelativeTo("host"),
	).Transform(spec.Raw())
	if err != nil {
		return fmt.Errorf("failed to transform driver root in CDI spec: %w", err)
	}

	// Update the spec to include only the minimum version necessary.
	minVersion, err := cdispec.MinimumRequiredVersion(spec.Raw())
	if err != nil {
		return fmt.Errorf("failed to get minimum required CDI spec version: %w", err)
	}
	spec.Raw().Version = minVersion
	klog.V(6).Infof("Write CDI spec: %s", specName)
	return cdi.cache.WriteSpec(spec.Raw(), specName)
}

func (cdi *CDIHandler) GetCommonEditsCached() (*cdiapi.ContainerEdits, error) {
	key := "commonEdits"
	if v, ok := cdi.specCache.Get(key); ok {
		edits := v.(*cdiapi.ContainerEdits)
		// Return a shallow copy so that cache entry consumer is less likely to
		// mutate the cache entry.
		clone := *edits
		return &clone, nil
	}

	t0 := time.Now()
	v, err := cdi.nvcdiClaim.GetCommonEdits()
	klog.V(7).Infof("t_cdi_get_common_edits %.3f s", time.Since(t0).Seconds())

	if err != nil {
		return nil, err
	}
	cdi.specCache.Set(key, v, time.Duration(5*time.Minute))
	// Return a shallow copy, see above.
	clone := *v
	return &clone, nil
}

func (cdi *CDIHandler) WarmupDevSpecCache(uuids []string) {
	for _, uuid := range uuids {
		_, err := cdi.GetDeviceSpecsByUUIDCached(uuid)
		if err != nil {
			klog.Warningf("Ignore error during cache warmup: GetDeviceSpecsByUUIDCached() failed: %s", err)
		}
	}
}

func (cdi *CDIHandler) GetDeviceSpecsByUUIDCached(uuid string) ([]cdispec.Device, error) {
	key := uuid
	if v, ok := cdi.specCache.Get(key); ok {
		devs := v.([]cdispec.Device)
		clone := make([]cdispec.Device, len(devs))
		copy(clone, devs)
		return clone, nil
	}

	t0 := time.Now()
	devs, err := cdi.nvcdiClaim.GetDeviceSpecsByID(uuid)
	klog.Infof("GetDeviceSpecsByID() called for %s", uuid)
	klog.V(6).Infof("t_cdi_get_specs_for_uuid %.3f s", time.Since(t0).Seconds())
	if err != nil {
		return nil, err
	}
	cdi.specCache.Set(key, devs, time.Duration(5*time.Minute))
	clone := make([]cdispec.Device, len(devs))
	copy(clone, devs)
	return clone, nil
}

// Note(JP): for a regular GPU, this canonical name is for example `gpu-0`, with
// the numerical suffix as of the time of writing reflecting the device minor.
// NVMLs' DeviceSetMigMode() is documented with 'This API may unbind or reset
// the device to activate the requested mode. Thus, the attributes associated
// with the device, such as minor number, might change. The caller of this API
// is expected to query such attributes again.' -- if the minor is indeed not
// necessarily stable, there may be problems associating this spec _long-term_
// with that name. That is an argument for always dynamically generating also
// full-GPU CDI spec during prepare() (or: to cache it, and re-generate it every
// now and then during this program's lifetime).
func (cdi *CDIHandler) CreateClaimSpecFile(claimUID string, preparedDevices PreparedDevices) error {
	// Generate those parts of the container spec that are not device-specific
	// (to inject e.g. driver library mounts and meta devices). Note that
	// `nvcdiDevice.GetCommonEdits()` may usually initialize nvsandboxutilslib
	// under the hood -- we now prevent that from happening by using
	// `nvcdi.FeatureDisableNvsandboxUtils` above.
	commonEdits, err := cdi.GetCommonEditsCached()
	if err != nil {
		return fmt.Errorf("failed to get common CDI spec edits: %w", err)
	}

	// Make sure that NVIDIA_VISIBLE_DEVICES is set to void to avoid the
	// nvidia-container-runtime honoring it in addition to the underlying
	// runtime honoring CDI.
	commonEdits.Env = append(
		commonEdits.Env,
		"NVIDIA_VISIBLE_DEVICES=void")

	var deviceSpecs []cdispec.Device
	for _, group := range preparedDevices {
		for _, dev := range group.Devices {
			uuid := ""

			// Construct claim-specific CDI device name in accordance with the
			// naming convention encoded in `GetClaimDeviceName()` below.
			dname := fmt.Sprintf("%s-%s", claimUID, dev.CanonicalName())

			var dspec cdispec.Device

			if dev.Type() == GpuDeviceType {
				uuid = dev.Gpu.Info.UUID
				// Get (copy of) cached CDI spec (is safe to be mutated below,
				// w/o compromising cache).
				dspecsgpu, err := cdi.GetDeviceSpecsByUUIDCached(uuid)
				if err != nil {
					return fmt.Errorf("unable to get device spec for %s: %w", dname, err)
				}
				dspec = dspecsgpu[0]
			}

			if dev.Type() == VfioDeviceType {
				// For now, just overwrite commonEdits (potentially multiple
				// times with the same data). Can we also use
				// `cdi.nvcdiDevice.GetCommonEdits()` here (wasn't done in the
				// original vfio PR)? Also: assume that all devices in
				// `preparedDevices` are vfio devices; a mixture isn't supported
				// by the current business logic and leads to unexpected
				// behavior.
				commonEdits = GetVfioCommonCDIContainerEdits()
				dspec = cdispec.Device{
					ContainerEdits: *GetVfioCDIContainerEdits(dev.Vfio.Info).ContainerEdits,
				}
			}

			if dev.Type() == PreparedMigDeviceType {
				// Here, get the 'parent dev node' part of the spec. THe spec
				// fragment for other dev nodes specific to this MIG device is
				// generated further below. One reason for doing things this way
				// is that `nvcdiDevice.GetDeviceSpecsByID(MIG_UUID)` may yield
				// an incomplete spec for MIG devices, see
				// https://github.com/NVIDIA/k8s-dra-driver-gpu/issues/787.
				uuid = dev.Mig.Created.parent.UUID
				// Get (copy of) cached device spec (is safe to be mutated below,
				// w/o compromising cache).
				dspecsmig, err := cdi.GetDeviceSpecsByUUIDCached(uuid)
				if err != nil {
					return fmt.Errorf("unable to get device spec for %s: %w", dname, err)
				}
				dspec = dspecsmig[0]

				devnodesForMig, err := cdi.GetDevNodesForMigDevice(dev.Mig.Created.parent.minor, int(dev.Mig.Created.GIID), int(dev.Mig.Created.CIID))
				if err != nil {
					return fmt.Errorf("failed to construct MIG device DeviceNode edits: %w", err)
				}
				klog.V(7).Infof("CDI spec: appending MIG device nodes")
				dspec.ContainerEdits.DeviceNodes = append(dspec.ContainerEdits.DeviceNodes, devnodesForMig...)
			}

			// The previous calls guarantee that at this point, `dspec` is a
			// list with precisely one item. Now, associate spec with
			// claim-specific device name generated above.
			dspec.Name = dname
			klog.V(7).Infof("Number of device nodes about to inject for device %s: %d", dname, len(dspec.ContainerEdits.DeviceNodes))
			deviceSpecs = append(deviceSpecs, dspec)

			// If there edits passed as part of the device config state (set on
			// the group), add them to the spec of each device in that group.
			if group.ConfigState.containerEdits != nil {
				deviceSpec := cdispec.Device{
					Name:           fmt.Sprintf("%s-%s", claimUID, dname),
					ContainerEdits: *group.ConfigState.containerEdits.ContainerEdits,
				}
				deviceSpecs = append(deviceSpecs, deviceSpec)
			}
		}
	}

	tws0 := time.Now()

	spec, err := spec.New(
		spec.WithVendor(cdiVendor),
		spec.WithClass(cdiClaimClass),
		spec.WithDeviceSpecs(deviceSpecs),
		spec.WithEdits(*commonEdits.ContainerEdits),
	)
	if err != nil {
		return fmt.Errorf("failed to create CDI spec: %w", err)
	}

	// Write the per-claim spec that was generated above to the filesystem. As
	// it is bound to a DRA ResourceClaim, it's transient (bound to the lifetime
	// of a container). Hence, Use the "transient spec" concept from CDI.
	specName := cdiapi.GenerateTransientSpecName(cdiVendor, cdiClaimClass, claimUID)
	klog.V(6).Infof("Writing CDI spec '%s' for claim '%s'", specName, claimUID)
	result := cdi.writeSpec(spec, specName)
	klog.V(6).Infof("t_gen_write_cdi_spec %.3f s", time.Since(tws0).Seconds())
	return result
}

func (cdi *CDIHandler) DeleteClaimSpecFile(claimUID string) error {
	specName := cdiapi.GenerateTransientSpecName(cdiVendor, cdiClaimClass, claimUID)
	klog.V(6).Infof("Delete CDI spec file: '%s', claim '%s'", specName, claimUID)
	return cdi.cache.RemoveSpec(specName)
}

// Philosophy: all devices to be injected into a container are defined in a
// single, transient CDI spec. This function returns the fully qualified
// identifier for a device defined in that spec. Example:
// k8s.gpu.nvidia.com/claim=dab5ab50-d59a-42a6-af16-cfd4628c0f7a-gpu-0
// That identifier can be used elsewhere, and _points to the spec_.
func (cdi *CDIHandler) GetClaimDeviceName(claimUID string, device *AllocatableDevice, containerEdits *cdiapi.ContainerEdits) string {
	return cdiparser.QualifiedName(cdiVendor, cdiClaimClass, fmt.Sprintf("%s-%s", claimUID, device.CanonicalName()))
}

// Construct and return the CDI `deviceNodes` specification for the two
// character devices `/dev/nvidia-caps/nvidia-cap<CIm>` and
// `/dev/nvidia-caps/nvidia-cap<GIm>` for a specific MIG device.
//
// Context: for containerized workload to see and use a specific MIG device, it
// needs to be able to open three character device nodes:
//
// 1) `/dev/nvidia<Pm>`, with <Pm> referring to the parent's minor. This exists
// on the host.
//
// 2) /dev/nvidia-caps/nvidia-cap<CIm> and /dev/nvidia-caps/nvidia-cap<GIm>,
// with <GIm> and <CIm> referring to the MIG GPU instance's and Compute
// instance's minor, respectively. For the the latter two device nodes it is
// sufficient to create them in the container (with proper cgroups permissions),
// without actually requiring the same device nodes to be explicitly created on
// the host. That is what is achieved below with the structure created in
// cdiDevNodeFromNVCapDevInfo().
func (cdi *CDIHandler) GetDevNodesForMigDevice(parentMinor int, giId int, ciId int) ([]*cdispec.DeviceNode, error) {
	gipath := fmt.Sprintf("%s/gpu%d/mig/gi%d/access", procNvCapsPath, parentMinor, giId)
	cipath := fmt.Sprintf("%s/gpu%d/mig/gi%d/ci%d/access", procNvCapsPath, parentMinor, giId, ciId)

	giCapsInfo, err := common.ParseNVCapDeviceInfo(gipath)
	if err != nil {
		return nil, fmt.Errorf("failed to parse GI capabilities file %s: %w", gipath, err)
	}

	ciCapsInfo, err := common.ParseNVCapDeviceInfo(cipath)
	if err != nil {
		return nil, fmt.Errorf("failed to parse CI capabilities file %s: %w", cipath, err)
	}

	devnodes := []*cdispec.DeviceNode{giCapsInfo.CDICharDevNode(), ciCapsInfo.CDICharDevNode()}
	return devnodes, nil
}
